'use client'

import { useRef, useEffect, useCallback } from 'react'

interface RealtimeWaveformProps {
  trackId: string
  isRecording: boolean
  isArmed: boolean
  currentTime: number
  duration: number
  width?: number
  height?: number
}

export default function RealtimeWaveform({ 
  trackId, 
  isRecording, 
  isArmed, 
  currentTime, 
  duration,
  width = 800,
  height = 80
}: RealtimeWaveformProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const animationFrameRef = useRef<number>()
  const audioContextRef = useRef<AudioContext>()
  const analyserRef = useRef<AnalyserNode>()
  const microphoneRef = useRef<MediaStreamAudioSourceNode>()
  const streamRef = useRef<MediaStream>()
  const waveformDataRef = useRef<Map<number, number>>(new Map())

  // æ¸…ç†å‡½æ•°
  const cleanup = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = undefined
    }

    if (microphoneRef.current) {
      microphoneRef.current.disconnect()
      microphoneRef.current = undefined
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = undefined
    }

    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = undefined
    }

    analyserRef.current = undefined
    waveformDataRef.current.clear()
  }, [])

  // å¼€å§‹éŸ³é¢‘é‡‡é›†
  const startCapture = useCallback(async () => {
    try {
      console.log(`ğŸµ Starting waveform capture for track ${trackId}`)
      console.log(`ğŸµ Recording state: ${isRecording}, Armed state: ${isArmed}`)
      
      // å¼ºåˆ¶æ¸…ç†ä¹‹å‰çš„èµ„æº
      cleanup()
      
      // æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        console.error('âŒ getUserMedia not supported in this browser')
        return
      }
      
      // è¯·æ±‚éº¦å…‹é£æƒé™
      console.log('ğŸ¤ Requesting microphone access...')
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 44100
        } 
      })
      streamRef.current = stream
      console.log('âœ… Microphone access granted')

      // åˆ›å»ºAudioContext - ä½¿ç”¨ç”¨æˆ·äº¤äº’ç¡®ä¿æ¿€æ´»
      console.log('ğŸ”Š Creating AudioContext...')
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)()
      console.log(`ğŸ”Š AudioContext state: ${audioContextRef.current.state}`)
      
      // å¼ºåˆ¶æ¢å¤AudioContext
      if (audioContextRef.current.state === 'suspended') {
        console.log('ğŸ”Š Resuming AudioContext...')
        await audioContextRef.current.resume()
        console.log(`ğŸ”Š AudioContext resumed, new state: ${audioContextRef.current.state}`)
      }

      // ç­‰å¾…AudioContextå®Œå…¨æ¿€æ´»
      if (audioContextRef.current.state !== 'running') {
        console.log('â° Waiting for AudioContext to become running...')
        await new Promise((resolve, reject) => {
          const timeout = setTimeout(() => reject(new Error('AudioContext timeout')), 5000)
          const checkState = () => {
            if (audioContextRef.current?.state === 'running') {
              clearTimeout(timeout)
              resolve(undefined)
            } else {
              setTimeout(checkState, 100)
            }
          }
          checkState()
        })
      }

      // åˆ›å»ºåˆ†æå™¨
      console.log('ğŸ“Š Creating analyser...')
      analyserRef.current = audioContextRef.current.createAnalyser()
      analyserRef.current.fftSize = 2048
      analyserRef.current.smoothingTimeConstant = 0.1 // æ›´å¿«çš„å“åº”
      console.log(`ğŸ“Š Analyser created, buffer length: ${analyserRef.current.frequencyBinCount}`)

      // è¿æ¥éº¦å…‹é£
      console.log('ğŸ”Œ Connecting microphone to analyser...')
      microphoneRef.current = audioContextRef.current.createMediaStreamSource(stream)
      microphoneRef.current.connect(analyserRef.current)

      console.log(`âœ… Audio capture initialized for track ${trackId}`)
      console.log(`ğŸµ Starting waveform drawing loop...`)
      
      // ç«‹å³å¼€å§‹ç»˜åˆ¶å¾ªç¯
      drawWaveform()
      
      // æµ‹è¯•éŸ³é¢‘æ•°æ®
      setTimeout(() => {
        if (analyserRef.current) {
          const testArray = new Uint8Array(analyserRef.current.frequencyBinCount)
          analyserRef.current.getByteTimeDomainData(testArray)
          const hasData = testArray.some(val => val !== 128)
          console.log(`ğŸ”¬ Audio data test: ${hasData ? 'DATA DETECTED' : 'NO DATA'}, Sample: [${testArray.slice(0, 5).join(',')}]`)
        }
      }, 1000)
      
    } catch (error) {
      console.error(`âŒ Failed to start audio capture for track ${trackId}:`, error)
      // æ˜¾ç¤ºå…·ä½“çš„é”™è¯¯ä¿¡æ¯
      if (error instanceof Error) {
        console.error(`âŒ Error details: ${error.name} - ${error.message}`)
      }
    }
  }, [trackId, isRecording, isArmed, cleanup])

  // ç»˜åˆ¶æ³¢å½¢
  const drawWaveform = useCallback(() => {
    if (!canvasRef.current) return

    const canvas = canvasRef.current
    const ctx = canvas.getContext('2d')
    if (!ctx) return

    // è®¾ç½®canvaså°ºå¯¸
    canvas.width = width
    canvas.height = height

    // æ¸…ç©ºcanvas
    ctx.fillStyle = '#0f172a' // slate-900
    ctx.fillRect(0, 0, width, height)

    // ç»˜åˆ¶ä¸­å¿ƒçº¿
    ctx.strokeStyle = '#334155' // slate-700
    ctx.lineWidth = 1
    ctx.beginPath()
    ctx.moveTo(0, height / 2)
    ctx.lineTo(width, height / 2)
    ctx.stroke()

    if (isRecording && isArmed && analyserRef.current) {
      // è·å–å½“å‰éŸ³é¢‘æ•°æ®
      const bufferLength = analyserRef.current.frequencyBinCount
      const dataArray = new Uint8Array(bufferLength)
      analyserRef.current.getByteTimeDomainData(dataArray)

      // è®¡ç®—RMSå€¼
      let sum = 0
      for (let i = 0; i < bufferLength; i++) {
        const normalized = (dataArray[i] - 128) / 128
        sum += normalized * normalized
      }
      const rms = Math.sqrt(sum / bufferLength)
      const amplitude = Math.min(rms * 4, 1) // æ”¾å¤§å¹¶é™åˆ¶

      // å°†æŒ¯å¹…æ•°æ®å­˜å‚¨åˆ°å½“å‰æ—¶é—´ä½ç½®
      const timeKey = Math.floor(currentTime * 10) // 0.1ç§’ç²¾åº¦
      waveformDataRef.current.set(timeKey, amplitude)

      // æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼Œä½†å‡å°‘é¢‘ç‡
      if (timeKey % 5 === 0) { // æ¯0.5ç§’è¾“å‡ºä¸€æ¬¡
        console.log(`ğŸµ Track ${trackId} - Time: ${currentTime.toFixed(1)}s, Amplitude: ${amplitude.toFixed(3)}, RMS: ${rms.toFixed(3)}, Data points: ${waveformDataRef.current.size}`)
        console.log(`ğŸµ Raw audio sample: [${Array.from(dataArray.slice(0, 5)).join(',')}]`)
      }
    }

    // ç»˜åˆ¶å†å²æ³¢å½¢æ•°æ®
    ctx.strokeStyle = '#10b981' // green-500
    ctx.fillStyle = 'rgba(16, 185, 129, 0.3)' // green-500 é€æ˜
    ctx.lineWidth = 2

    const pixelsPerSecond = width / duration
    const sortedEntries = Array.from(waveformDataRef.current.entries()).sort((a, b) => a[0] - b[0])
    
    if (sortedEntries.length > 0) {
      ctx.beginPath()
      let pathStarted = false
      
      // ç»˜åˆ¶ä¸ŠåŠéƒ¨åˆ†æ³¢å½¢
      for (const [timeKey, amplitude] of sortedEntries) {
        const time = timeKey / 10 // è½¬å›ç§’
        const x = time * pixelsPerSecond
        
        if (x >= 0 && x <= width) {
          const centerY = height / 2
          const amplitudePixels = amplitude * (height / 2) * 0.8 // 80%çš„é«˜åº¦ç”¨äºæ³¢å½¢
          const topY = centerY - amplitudePixels
          
          if (!pathStarted) {
            ctx.moveTo(x, topY)
            pathStarted = true
          } else {
            ctx.lineTo(x, topY)
          }
        }
      }
      
      // ç»˜åˆ¶ä¸‹åŠéƒ¨åˆ†æ³¢å½¢ï¼ˆé•œåƒï¼‰
      for (let i = sortedEntries.length - 1; i >= 0; i--) {
        const [timeKey, amplitude] = sortedEntries[i]
        const time = timeKey / 10
        const x = time * pixelsPerSecond
        
        if (x >= 0 && x <= width) {
          const centerY = height / 2
          const amplitudePixels = amplitude * (height / 2) * 0.8
          const bottomY = centerY + amplitudePixels
          ctx.lineTo(x, bottomY)
        }
      }
      
      ctx.closePath()
      ctx.fill()
      ctx.stroke()
    }

    // ç»˜åˆ¶å½“å‰å½•éŸ³ä½ç½®æŒ‡ç¤ºå™¨
    if (isRecording && isArmed) {
      const currentX = currentTime * pixelsPerSecond
      if (currentX >= 0 && currentX <= width) {
        ctx.strokeStyle = '#ef4444' // red-500
        ctx.lineWidth = 2
        ctx.beginPath()
        ctx.moveTo(currentX, 0)
        ctx.lineTo(currentX, height)
        ctx.stroke()
      }
    }

    // ç»§ç»­åŠ¨ç”»å¾ªç¯
    if (isRecording && isArmed) {
      animationFrameRef.current = requestAnimationFrame(drawWaveform)
    }
  }, [width, height, duration, currentTime, isRecording, isArmed, trackId])

  // ç›‘å¬å½•éŸ³çŠ¶æ€å˜åŒ–
  useEffect(() => {
    console.log(`ğŸµ Track ${trackId} - Effect triggered: isRecording=${isRecording}, isArmed=${isArmed}`)
    
    if (isRecording && isArmed) {
      console.log(`ğŸµ Track ${trackId} - Starting capture...`)
      startCapture()
    } else {
      console.log(`ğŸµ Track ${trackId} - Cleaning up...`)
      cleanup()
    }

    return cleanup
  }, [isRecording, isArmed, startCapture, cleanup, trackId])

  // ç›‘å¬æ—¶é—´å˜åŒ–ï¼Œé‡ç»˜æ³¢å½¢
  useEffect(() => {
    if (!isRecording && canvasRef.current) {
      // éå½•éŸ³çŠ¶æ€ä¸‹ä¹Ÿè¦ç»˜åˆ¶å·²æœ‰çš„æ³¢å½¢æ•°æ®
      drawWaveform()
    }
  }, [currentTime, drawWaveform, isRecording])

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†
  useEffect(() => {
    return cleanup
  }, [cleanup])

  return (
    <div className="relative w-full">
      <canvas
        ref={canvasRef}
        className="w-full h-full bg-slate-900 rounded"
        style={{ height: `${height}px` }}
      />
      
      {/* è°ƒè¯•ä¿¡æ¯ */}
      {process.env.NODE_ENV === 'development' && (
        <div className="absolute top-1 left-1 text-xs text-slate-400 bg-black bg-opacity-50 px-1 rounded">
          T{trackId}: {isRecording && isArmed ? 'REC' : 'IDLE'} | 
          Data: {waveformDataRef.current.size} | 
          Time: {currentTime.toFixed(1)}s |
          Audio: {audioContextRef.current?.state || 'none'}
        </div>
      )}
      
      {/* éº¦å…‹é£æƒé™æµ‹è¯•æŒ‰é’® */}
      {process.env.NODE_ENV === 'development' && !audioContextRef.current && (
        <button 
          onClick={startCapture}
          className="absolute top-1 right-1 bg-blue-600 text-white px-2 py-1 rounded text-xs"
        >
          Test Mic
        </button>
      )}
    </div>
  )
}
