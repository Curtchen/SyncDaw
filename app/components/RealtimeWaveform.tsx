'use client'

import { useEffect, useRef, useCallback } from 'react'

interface WaveformBucket {
  min: number
  max: number
  rms: number
}

interface RealtimeWaveformProps {
  isRecording: boolean
  isArmed: boolean
  trackId: string
  width: number
  height: number
  currentTime: number
  duration: number
}

export default function RealtimeWaveform({
  isRecording,
  isArmed,
  trackId,
  width,
  height,
  currentTime,
  duration
}: RealtimeWaveformProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const animationRef = useRef<number | null>(null)
  // å­˜å‚¨åˆ†æ¡¶åçš„æ³¢å½¢æ•°æ®ï¼šæ—¶é—´æˆ³ -> æ³¢å½¢æ¡¶æ•°ç»„
  const waveformBucketsRef = useRef<Map<number, WaveformBucket[]>>(new Map())
  const recordingStartTimeRef = useRef<number>(0)
  // å½“å‰å®æ—¶éŸ³é¢‘æ•°æ®ç¼“å†²åŒº
  const audioBufferRef = useRef<Float32Array>(new Float32Array(2048))

  // å°†éŸ³é¢‘æ ·æœ¬åˆ†æ¡¶ï¼Œæå–å³°å€¼
  const createWaveformBuckets = (audioData: Float32Array, bucketsCount: number): WaveformBucket[] => {
    const buckets: WaveformBucket[] = []
    const samplesPerBucket = Math.floor(audioData.length / bucketsCount)
    
    for (let i = 0; i < bucketsCount; i++) {
      const start = i * samplesPerBucket
      const end = Math.min(start + samplesPerBucket, audioData.length)
      
      let min = 1
      let max = -1
      let sum = 0
      let count = 0
      
      for (let j = start; j < end; j++) {
        const sample = audioData[j]
        min = Math.min(min, sample)
        max = Math.max(max, sample)
        sum += sample * sample
        count++
      }
      
      const rms = count > 0 ? Math.sqrt(sum / count) : 0
      buckets.push({ min, max, rms })
    }
    
    return buckets
  }

  // ç»˜åˆ¶æ³¢å½¢
  const drawWaveform = useCallback(() => {
    const canvas = canvasRef.current
    if (!canvas) return

    const ctx = canvas.getContext('2d')
    if (!ctx) return

    // æ¸…ç©ºç”»å¸ƒ
    ctx.fillStyle = '#1e293b' // slate-800
    ctx.fillRect(0, 0, width, height)

    // ç»˜åˆ¶ç½‘æ ¼çº¿
    ctx.strokeStyle = 'rgba(100, 116, 139, 0.15)'
    ctx.lineWidth = 1

    // æ—¶é—´ç½‘æ ¼çº¿
    const pixelsPerSecond = width / Math.max(duration, 1)
    for (let i = 0; i <= duration; i++) {
      const x = i * pixelsPerSecond
      ctx.beginPath()
      ctx.moveTo(x, 0)
      ctx.lineTo(x, height)
      ctx.stroke()
    }

    // æ°´å¹³ä¸­å¿ƒçº¿
    const centerY = height / 2
    ctx.strokeStyle = 'rgba(100, 116, 139, 0.3)'
    ctx.beginPath()
    ctx.moveTo(0, centerY)
    ctx.lineTo(width, centerY)
    ctx.stroke()

    // ç»˜åˆ¶æ’­æ”¾å¤´
    if (currentTime >= 0) {
      const playheadX = (currentTime / Math.max(duration, 1)) * width
      ctx.strokeStyle = '#ef4444' // red-500
      ctx.lineWidth = 2
      ctx.beginPath()
      ctx.moveTo(playheadX, 0)
      ctx.lineTo(playheadX, height)
      ctx.stroke()
    }

    // ç»˜åˆ¶å·²å½•åˆ¶çš„æ³¢å½¢æ•°æ®
    drawRecordedWaveforms(ctx, pixelsPerSecond, centerY)

    // ç»˜åˆ¶å®æ—¶æ³¢å½¢
    if (isRecording && isArmed) {
      drawRealtimeWaveform(ctx, pixelsPerSecond, centerY)
    }

    // çŠ¶æ€æç¤º
    if (!isRecording && isArmed) {
      ctx.fillStyle = 'rgba(255, 165, 0, 0.7)'
      ctx.font = '14px system-ui'
      ctx.textAlign = 'center'
      ctx.fillText('READY TO RECORD', width / 2, height / 2 - 10)
      ctx.fillStyle = 'rgba(255, 165, 0, 0.5)'
      ctx.font = '12px system-ui'
      ctx.fillText('Click record to start capturing audio', width / 2, height / 2 + 10)
    } else if (!isArmed) {
      ctx.fillStyle = 'rgba(100, 116, 139, 0.6)'
      ctx.font = '12px system-ui'
      ctx.textAlign = 'center'
      ctx.fillText('Arm track to enable recording', width / 2, height / 2)
    }
  }, [width, height, currentTime, duration, isRecording, isArmed])

  // ç»˜åˆ¶å®æ—¶æ³¢å½¢
  const drawRealtimeWaveform = (ctx: CanvasRenderingContext2D, pixelsPerSecond: number, centerY: number) => {
    if (!analyserRef.current) return

    // è·å–å½“å‰éŸ³é¢‘æ•°æ®
    analyserRef.current.getFloatTimeDomainData(audioBufferRef.current)
    
    // è®¡ç®—å½“å‰å½•éŸ³ä½ç½®
    const recordingTime = currentTime - recordingStartTimeRef.current
    const currentX = recordingTime * pixelsPerSecond
    
    if (currentX < 0 || currentX >= width) return

    // å°†éŸ³é¢‘æ•°æ®åˆ†æ¡¶ - ç”¨è¾ƒå°‘çš„æ¡¶æ•°æ¥æ˜¾ç¤ºå®æ—¶æ•°æ®
    const bucketsCount = Math.min(100, width / 4) // æ¯4åƒç´ ä¸€ä¸ªæ¡¶
    const buckets = createWaveformBuckets(audioBufferRef.current, bucketsCount)
    
    // ç»˜åˆ¶æ³¢å½¢
    ctx.strokeStyle = '#22c55e' // green-500
    ctx.fillStyle = 'rgba(34, 197, 94, 0.3)' // green with alpha
    ctx.lineWidth = 1

    // ç»˜åˆ¶ä¸Šä¸‹è¾¹ç•Œçº¿å’Œå¡«å……
    const bucketWidth = Math.max(1, Math.floor(pixelsPerSecond / 10)) // 0.1ç§’çš„å®½åº¦
    
    buckets.forEach((bucket, index) => {
      const x = currentX + (index * bucketWidth) - (bucketsCount * bucketWidth / 2)
      
      if (x >= 0 && x < width) {
        const maxY = centerY - (bucket.max * (height * 0.4))
        const minY = centerY - (bucket.min * (height * 0.4))
        
        // å¡«å……åŒºåŸŸ
        ctx.fillRect(x, Math.min(maxY, centerY), bucketWidth, Math.abs(maxY - minY))
        
        // è¾¹ç•Œçº¿
        ctx.beginPath()
        ctx.moveTo(x, maxY)
        ctx.lineTo(x + bucketWidth, maxY)
        ctx.moveTo(x, minY)
        ctx.lineTo(x + bucketWidth, minY)
        ctx.stroke()
      }
    })

    // å­˜å‚¨æ³¢å½¢æ•°æ®åˆ°å†å²è®°å½•ï¼ˆæ¯0.1ç§’å­˜å‚¨ä¸€æ¬¡ï¼‰
    const timeKey = Math.floor(recordingTime * 10) / 10
    if (timeKey >= 0 && buckets.length > 0) {
      waveformBucketsRef.current.set(timeKey, [...buckets])
    }
  }

  // ç»˜åˆ¶å·²å½•åˆ¶çš„æ³¢å½¢
  const drawRecordedWaveforms = (ctx: CanvasRenderingContext2D, pixelsPerSecond: number, centerY: number) => {
    ctx.strokeStyle = '#22c55e'
    ctx.fillStyle = 'rgba(34, 197, 94, 0.2)'
    ctx.lineWidth = 1

    Array.from(waveformBucketsRef.current.entries()).forEach(([timeKey, buckets]) => {
      const startX = timeKey * pixelsPerSecond
      
      if (startX >= -100 && startX < width + 100) { // ç»˜åˆ¶ç¨å¾®è¶…å‡ºå±å¹•çš„éƒ¨åˆ†
        const bucketWidth = Math.max(1, pixelsPerSecond / buckets.length)
        
        buckets.forEach((bucket, index) => {
          const x = startX + (index * bucketWidth)
          
          if (x >= -bucketWidth && x < width + bucketWidth) {
            const maxY = centerY - (bucket.max * (height * 0.4))
            const minY = centerY - (bucket.min * (height * 0.4))
            
            // å¡«å……
            ctx.fillRect(x, Math.min(maxY, centerY), bucketWidth, Math.abs(maxY - minY))
            
            // è½®å»“
            ctx.strokeRect(x, Math.min(maxY, minY), bucketWidth, Math.abs(maxY - minY))
          }
        })
      }
    })
  }

  // å¯åŠ¨å½•éŸ³
  const startRecording = async () => {
    try {
      console.log('ğŸ¤ å¯åŠ¨å®æ—¶éŸ³é¢‘å½•åˆ¶:', trackId)
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 44100,
          channelCount: 1
        }
      })

      streamRef.current = stream
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)()
      
      // ç¡®ä¿éŸ³é¢‘ä¸Šä¸‹æ–‡å¤„äºè¿è¡ŒçŠ¶æ€
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume()
      }
      
      analyserRef.current = audioContextRef.current.createAnalyser()
      analyserRef.current.fftSize = 2048 // 1024ä¸ªæ—¶åŸŸæ ·æœ¬
      analyserRef.current.smoothingTimeConstant = 0 // æ— å¹³æ»‘ï¼Œè·å–åŸå§‹æ•°æ®
      
      const source = audioContextRef.current.createMediaStreamSource(stream)
      source.connect(analyserRef.current)

      recordingStartTimeRef.current = currentTime
      audioBufferRef.current = new Float32Array(analyserRef.current.fftSize)

      console.log('âœ… éŸ³é¢‘å½•åˆ¶å¯åŠ¨æˆåŠŸ, é‡‡æ ·ç‡:', audioContextRef.current.sampleRate)
      
      // å¼€å§‹åŠ¨ç”»å¾ªç¯
      const animate = () => {
        drawWaveform()
        if (isRecording && isArmed && analyserRef.current) {
          animationRef.current = requestAnimationFrame(animate)
        }
      }
      animate()

    } catch (error) {
      console.error('âŒ å¯åŠ¨å½•éŸ³å¤±è´¥:', error)
      alert('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æµè§ˆå™¨æƒé™è®¾ç½®')
    }
  }

  // åœæ­¢å½•éŸ³
  const stopRecording = () => {
    console.log('ğŸ›‘ åœæ­¢å½•éŸ³:', trackId)
    
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current)
      animationRef.current = null
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }

    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    analyserRef.current = null
    
    // æœ€åç»˜åˆ¶ä¸€æ¬¡ï¼Œæ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
    drawWaveform()
  }

  // ç›‘å¬å½•éŸ³çŠ¶æ€å˜åŒ–
  useEffect(() => {
    if (isRecording && isArmed) {
      startRecording()
    } else {
      stopRecording()
    }

    return () => stopRecording()
  }, [isRecording, isArmed, trackId])

  // ç›‘å¬å°ºå¯¸å˜åŒ–
  useEffect(() => {
    const canvas = canvasRef.current
    if (canvas) {
      // è®¾ç½®å®é™…åƒç´ å°ºå¯¸
      const devicePixelRatio = window.devicePixelRatio || 1
      canvas.width = width * devicePixelRatio
      canvas.height = height * devicePixelRatio
      
      const ctx = canvas.getContext('2d')
      if (ctx) {
        ctx.scale(devicePixelRatio, devicePixelRatio)
      }
      
      // è®¾ç½®CSSå°ºå¯¸
      canvas.style.width = `${width}px`
      canvas.style.height = `${height}px`
      
      drawWaveform()
    }
  }, [width, height, drawWaveform])

  // ç›‘å¬æ—¶é—´å˜åŒ–
  useEffect(() => {
    if (!isRecording) {
      drawWaveform()
    }
  }, [currentTime, drawWaveform])

  return (
    <canvas
      ref={canvasRef}
      className="w-full h-full block"
      style={{ 
        background: '#1e293b',
        imageRendering: 'pixelated'
      }}
    />
  )
}
