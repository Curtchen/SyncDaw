'use client'

import { useEffect, useRef } from 'react'

interface TestWaveformProps {
  isRecording: boolean
  isArmed: boolean
  trackId: string
  width: number
  height: number
  currentTime: number
  duration: number
}

export default function TestWaveform({
  isRecording,
  isArmed,
  trackId,
  width,
  height,
  currentTime,
  duration
}: TestWaveformProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null)
  // å­˜å‚¨å½•éŸ³è¿‡ç¨‹ä¸­çš„éŸ³é¢‘æ•°æ®å†å²
  const waveformHistoryRef = useRef<Map<number, number[]>>(new Map())
  // å½“å‰å®æ—¶éŸ³é¢‘æ•°æ®
  const currentWaveformRef = useRef<number[]>([])
  const animationFrameRef = useRef<number>()
  // éŸ³é¢‘ç›¸å…³å¼•ç”¨
  const audioContextRef = useRef<AudioContext>()
  const analyserRef = useRef<AnalyserNode>()
  const microphoneRef = useRef<MediaStreamAudioSourceNode>()
  const streamRef = useRef<MediaStream>()
  const recordingStartTimeRef = useRef<number>(0)

  useEffect(() => {
    console.log(`ğŸ¤ RealTimeWaveform effect - trackId: ${trackId}, isRecording: ${isRecording}, isArmed: ${isArmed}`)
    
    if (isRecording && isArmed) {
      startRealTimeRecording()
    } else {
      stopRealTimeRecording()
    }

    return () => stopRealTimeRecording()
  }, [isRecording, isArmed])

  // ç›‘å¬canvaså°ºå¯¸å˜åŒ–ï¼Œé‡æ–°ç»˜åˆ¶
  useEffect(() => {
    const canvas = canvasRef.current
    if (canvas) {
      canvas.width = width
      canvas.height = height
      draw() // é‡æ–°ç»˜åˆ¶
    }
  }, [width, height])

  const startRealTimeRecording = async () => {
    console.log('ğŸ¤ Starting REAL-TIME recording for track', trackId)
    
    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 44100
        }
      })
      
      streamRef.current = stream
      audioContextRef.current = new AudioContext()
      analyserRef.current = audioContextRef.current.createAnalyser()
      
      // é…ç½®analyserç”¨äºæ—¶åŸŸåˆ†æ
      analyserRef.current.fftSize = 2048  // è·å¾—1024ä¸ªæ—¶åŸŸæ ·æœ¬ç‚¹
      analyserRef.current.smoothingTimeConstant = 0 // ä¸å¹³æ»‘ï¼Œè·å¾—åŸå§‹æ•°æ®
      
      microphoneRef.current = audioContextRef.current.createMediaStreamSource(stream)
      microphoneRef.current.connect(analyserRef.current)
      
      // æ¸…ç©ºå†å²æ•°æ®
      waveformHistoryRef.current.clear()
      currentWaveformRef.current = []
      recordingStartTimeRef.current = currentTime
      
      console.log('âœ… Real-time audio capture started')
      captureRealTimeAudio()
      
    } catch (error) {
      console.error('âŒ Error starting real-time recording:', error)
      // å¦‚æœè·å–éŸ³é¢‘å¤±è´¥ï¼Œæ˜¾ç¤ºé”™è¯¯çŠ¶æ€
    }
  }

  const stopRealTimeRecording = () => {
    console.log('ğŸ¤ Stopping REAL-TIME recording for track', trackId)
    
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = undefined
    }
    
    if (microphoneRef.current) {
      microphoneRef.current.disconnect()
      microphoneRef.current = undefined
    }
    
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = undefined
    }
    
    analyserRef.current = undefined
  }

  const captureRealTimeAudio = () => {
    if (!analyserRef.current || !isRecording || !isArmed) {
      return
    }

    // è·å–æ—¶åŸŸæ•°æ®
    const bufferLength = analyserRef.current.fftSize
    const timeData = new Float32Array(bufferLength)
    analyserRef.current.getFloatTimeDomainData(timeData)

    // ä¸‹é‡‡æ ·ä»¥é€‚åº”å±å¹•æ˜¾ç¤ºï¼ˆæ¯100ä¸ªæ ·æœ¬å–ä¸€ä¸ªï¼‰
    const samplesPerPixel = Math.max(1, Math.floor(bufferLength / 200))
    const displaySamples: number[] = []
    
    for (let i = 0; i < bufferLength; i += samplesPerPixel) {
      // è®¡ç®—è¿™ä¸ªåŒºé—´å†…çš„RMSå€¼
      let sum = 0
      let count = 0
      for (let j = i; j < Math.min(i + samplesPerPixel, bufferLength); j++) {
        sum += timeData[j] * timeData[j]
        count++
      }
      const rms = Math.sqrt(sum / count)
      displaySamples.push(rms)
    }

    // å­˜å‚¨å½“å‰æ³¢å½¢æ•°æ®
    currentWaveformRef.current = displaySamples
    
    // å°†å½“å‰æ³¢å½¢å­˜å‚¨åˆ°å†å²è®°å½•ä¸­ï¼ˆä»¥æ—¶é—´ä¸ºé”®ï¼‰
    const recordingTime = currentTime - recordingStartTimeRef.current
    if (recordingTime >= 0) {
      const timeKey = Math.floor(recordingTime * 10) / 10 // 0.1ç§’ç²¾åº¦
      waveformHistoryRef.current.set(timeKey, [...displaySamples])
      
      // å†…å­˜ç®¡ç†ï¼šé™åˆ¶å†å²è®°å½•å¤§å°
      if (waveformHistoryRef.current.size > 300) { // ä¿æŒ30ç§’çš„å†å²
        const oldestKey = Math.min(...Array.from(waveformHistoryRef.current.keys()))
        waveformHistoryRef.current.delete(oldestKey)
      }
    }

    // å®æ—¶ç»˜åˆ¶
    draw()

    // ç»§ç»­æ•è·
    if (isRecording && isArmed) {
      animationFrameRef.current = requestAnimationFrame(captureRealTimeAudio)
    }
  }

  const draw = () => {
    const canvas = canvasRef.current
    if (!canvas) return

    const ctx = canvas.getContext('2d')
    if (!ctx) return

    // Clear with solid background
    ctx.fillStyle = '#0f172a'
    ctx.fillRect(0, 0, width, height)

    // Draw center line
    const centerY = height / 2
    ctx.strokeStyle = 'rgba(100, 116, 139, 0.3)'
    ctx.lineWidth = 1
    ctx.beginPath()
    ctx.moveTo(0, centerY)
    ctx.lineTo(width, centerY)
    ctx.stroke()

    if (isRecording && isArmed) {
      // ç»˜åˆ¶å®æ—¶éŸ³é¢‘æ³¢å½¢
      const currentTime = new Date().getTime()
      const recordingDuration = (currentTime - recordingStartTimeRef.current) / 1000
      
      // ç»˜åˆ¶å†å²æ³¢å½¢æ•°æ®
      ctx.strokeStyle = '#22c55e'
      ctx.fillStyle = 'rgba(34, 197, 94, 0.2)'
      ctx.lineWidth = 1

      // è®¡ç®—æ—¶é—´çª—å£ - æ˜¾ç¤ºæœ€è¿‘çš„å½•éŸ³æ•°æ®
      const pixelsPerSecond = width / Math.max(duration, 10) // è‡³å°‘æ˜¾ç¤º10ç§’
      
      // ç»˜åˆ¶æ‰€æœ‰å†å²æ³¢å½¢
      Array.from(waveformHistoryRef.current.entries()).forEach(([timeKey, samples]) => {
        const x = timeKey * pixelsPerSecond
        if (x >= 0 && x < width) {
          // ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹ç»˜åˆ¶æ³¢å½¢
          ctx.beginPath()
          for (let i = 0; i < samples.length; i++) {
            const sampleX = x + (i / samples.length) * (pixelsPerSecond * 0.1) // 0.1ç§’çš„å®½åº¦
            const amplitude = samples[i]
            const y = centerY - (amplitude * height * 0.4)
            
            if (i === 0) {
              ctx.moveTo(sampleX, y)
            } else {
              ctx.lineTo(sampleX, y)
            }
          }
          ctx.stroke()
        }
      })

      // ç»˜åˆ¶å½“å‰å®æ—¶æ•°æ®ï¼ˆåœ¨æœ€å³ä¾§ï¼‰
      if (currentWaveformRef.current.length > 0) {
        ctx.strokeStyle = '#10b981'
        ctx.lineWidth = 2
        
        const currentX = recordingDuration * pixelsPerSecond
        ctx.beginPath()
        
        for (let i = 0; i < currentWaveformRef.current.length; i++) {
          const x = currentX + (i / currentWaveformRef.current.length) * 20 // å½“å‰æ•°æ®å ç”¨20åƒç´ å®½åº¦
          const amplitude = currentWaveformRef.current[i]
          const y = centerY - (amplitude * height * 0.4)
          
          if (i === 0) {
            ctx.moveTo(x, y)
          } else {
            ctx.lineTo(x, y)
          }
        }
        ctx.stroke()
      }

      // æ˜¾ç¤ºå½•éŸ³çŠ¶æ€
      ctx.fillStyle = '#22c55e'
      ctx.font = '12px monospace'
      ctx.textAlign = 'right'
      const historyCount = waveformHistoryRef.current.size
      const currentSamples = currentWaveformRef.current.length
      ctx.fillText(`â— RECORDING - History: ${historyCount}, Current: ${currentSamples}`, width - 10, 20)
      
    } else if (!isRecording && isArmed) {
      // Show armed state
      ctx.fillStyle = '#22c55e'
      ctx.font = '16px monospace'
      ctx.textAlign = 'center'
      ctx.fillText('â— ARMED - Ready to Record (REAL AUDIO)', width / 2, centerY)
    } else if (!isArmed) {
      // Show disarmed state
      ctx.fillStyle = '#64748b'
      ctx.font = '14px monospace'
      ctx.textAlign = 'center'
      ctx.fillText('Track not armed - Click record button to arm', width / 2, centerY)
    }
    
    // ç»˜åˆ¶å½“å‰æ—¶é—´æŒ‡ç¤ºå™¨ï¼ˆçº¢è‰²è¿›åº¦æ¡ï¼‰
    const timelineX = (currentTime / duration) * width
    if (timelineX >= 0 && timelineX <= width) {
      ctx.strokeStyle = '#ef4444'
      ctx.lineWidth = 2
      ctx.beginPath()
      ctx.moveTo(timelineX, 0)
      ctx.lineTo(timelineX, height)
      ctx.stroke()
    }
  }

  return (
    <canvas
      ref={canvasRef}
      width={width}
      height={height}
      className="absolute top-0 left-0 w-full h-full"
      style={{ 
        width: `${width}px`, 
        height: `${height}px`
      }}
    />
  )
}
