'use client'

import { useEffect, useRef } from 'react'

interface TimedomainWaveformProps {
  isRecording: boolean
  isArmed: boolean
  trackId: string
  width: number
  height: number
  currentTime: number
  duration: number
}

export default function TimedomainWaveform({
  isRecording,
  isArmed,
  trackId,
  width,
  height,
  currentTime,
  duration
}: TimedomainWaveformProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const audioContextRef = useRef<AudioContext>()
  const analyserRef = useRef<AnalyserNode>()
  const microphoneRef = useRef<MediaStreamAudioSourceNode>()
  const streamRef = useRef<MediaStream>()
  const animationFrameRef = useRef<number>()
  
  // å­˜å‚¨å®Œæ•´çš„æ—¶åŸŸæ³¢å½¢æ•°æ® - æ¯ä¸ªæ—¶é—´ç‚¹å­˜å‚¨ä¸€ä¸ªæ ·æœ¬æ•°ç»„
  const waveformBufferRef = useRef<Map<number, Float32Array>>(new Map())
  const recordingStartTimeRef = useRef<number>(0)

  useEffect(() => {
    if (isRecording && isArmed) {
      recordingStartTimeRef.current = currentTime
      startRecording()
    } else {
      stopRecording()
    }

    return () => stopRecording()
  }, [isRecording, isArmed])

  // ç»˜åˆ¶å¾ªç¯
  useEffect(() => {
    draw()
    
    if (isRecording && isArmed) {
      const drawLoop = () => {
        draw()
        animationFrameRef.current = requestAnimationFrame(drawLoop)
      }
      animationFrameRef.current = requestAnimationFrame(drawLoop)
    }
    
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
    }
  }, [isRecording, isArmed, currentTime, width, height])

  const startRecording = async () => {
    console.log('ğŸ¤ Starting timedomain waveform recording for track', trackId)
    
    try {
      if (!navigator.mediaDevices?.getUserMedia) {
        console.error('âŒ getUserMedia not supported')
        startTestMode()
        return
      }
      
      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)()
      }
      
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume()
      }
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 44100
        }
      })
      
      streamRef.current = stream
      
      // åˆ›å»ºåˆ†æå™¨èŠ‚ç‚¹ - ä¸“é—¨ç”¨äºæ—¶åŸŸåˆ†æ
      analyserRef.current = audioContextRef.current.createAnalyser()
      analyserRef.current.fftSize = 4096 // æ›´å¤§çš„FFTè·å¾—æ›´å¥½çš„æ—¶åŸŸåˆ†è¾¨ç‡
      analyserRef.current.smoothingTimeConstant = 0.0 // ä¸å¹³æ»‘
      
      microphoneRef.current = audioContextRef.current.createMediaStreamSource(stream)
      microphoneRef.current.connect(analyserRef.current)
      
      // æ¸…é™¤ä¹‹å‰çš„æ•°æ®
      waveformBufferRef.current.clear()
      
      console.log('âœ… Timedomain recording started with fftSize:', analyserRef.current.fftSize)
      captureTimeDomainData()
    } catch (err) {
      console.error('âŒ Error starting timedomain recording:', err)
      startTestMode()
    }
  }

  const captureTimeDomainData = () => {
    if (!analyserRef.current || !isRecording) return
    
    const bufferLength = analyserRef.current.fftSize
    const dataArray = new Float32Array(bufferLength)
    
    const capture = () => {
      if (!analyserRef.current || !isRecording) return
      
      // è·å–æ—¶åŸŸæ•°æ®ï¼ˆè¿™æ˜¯çœŸæ­£çš„éŸ³é¢‘æ³¢å½¢ï¼ï¼‰
      analyserRef.current.getFloatTimeDomainData(dataArray)
      
      // è®¡ç®—å½“å‰ç›¸å¯¹äºå½•éŸ³å¼€å§‹çš„æ—¶é—´
      const relativeTime = currentTime - recordingStartTimeRef.current
      
      if (relativeTime >= 0) {
        // å°†æ•´ä¸ªæ—¶åŸŸæ•°æ®å­˜å‚¨èµ·æ¥ï¼Œç²¾ç¡®åˆ°0.01ç§’
        const timeKey = Math.floor(relativeTime * 100) / 100
        
        // å¤åˆ¶æ•°æ®ï¼ˆå› ä¸ºdataArrayä¼šè¢«é‡ç”¨ï¼‰
        const waveformSlice = new Float32Array(dataArray)
        waveformBufferRef.current.set(timeKey, waveformSlice)
        
        // å†…å­˜ç®¡ç† - åªä¿ç•™æœ€è¿‘çš„æ•°æ®
        if (waveformBufferRef.current.size > 1000) { // çº¦10ç§’çš„æ•°æ®
          const oldestKey = Math.min(...Array.from(waveformBufferRef.current.keys()))
          waveformBufferRef.current.delete(oldestKey)
        }
        
        // è®¡ç®—ä¿¡å·å¼ºåº¦å¹¶æ·»åŠ æ›´è¯¦ç»†çš„è°ƒè¯•
        let rms = 0
        let peak = 0
        let nonZeroSamples = 0
        for (let i = 0; i < dataArray.length; i++) {
          const abs = Math.abs(dataArray[i])
          rms += dataArray[i] * dataArray[i]
          if (abs > peak) peak = abs
          if (abs > 0.001) nonZeroSamples++
        }
        rms = Math.sqrt(rms / dataArray.length)
        
        // æ›´é¢‘ç¹çš„è°ƒè¯•è¾“å‡º
        if (waveformBufferRef.current.size % 10 === 0) {
          console.log(`ğŸµ Chunk ${waveformBufferRef.current.size}: Peak=${peak.toFixed(4)}, RMS=${rms.toFixed(4)}, NonZero=${nonZeroSamples}/${dataArray.length}`)
          
          // è¾“å‡ºå‰å‡ ä¸ªæ ·æœ¬å€¼ç”¨äºè°ƒè¯•
          const firstSamples = Array.from(dataArray.slice(0, 10)).map(v => v.toFixed(3)).join(',')
          console.log(`First 10 samples: [${firstSamples}]`)
        }
        
        // å¦‚æœæ£€æµ‹åˆ°ä¿¡å·ï¼Œä¹Ÿç«‹å³è¾“å‡º
        if (peak > 0.01) {
          console.log(`ğŸ”Š Strong signal! Peak=${peak.toFixed(4)}, RMS=${rms.toFixed(4)}`)
        }
      }
      
      // ç»§ç»­æ•è· - æ›´é¢‘ç¹çš„é‡‡æ ·
      if (isRecording) {
        requestAnimationFrame(capture) // ä½¿ç”¨requestAnimationFrameè·å¾—æ›´å¹³æ»‘çš„é‡‡æ ·
      }
    }
    
    capture()
  }

  const startTestMode = () => {
    console.log('ğŸ§ª Starting test mode for timedomain waveform')
    
    const generateTestWaveform = () => {
      if (!isRecording) return
      
      const relativeTime = currentTime - recordingStartTimeRef.current
      if (relativeTime >= 0) {
        const timeKey = Math.floor(relativeTime * 100) / 100
        
        // ç”Ÿæˆæµ‹è¯•æ—¶åŸŸæ³¢å½¢æ•°æ®
        const testSamples = new Float32Array(1024)
        for (let i = 0; i < testSamples.length; i++) {
          const t = relativeTime + (i / 44100) // æ¨¡æ‹Ÿ44.1kHzé‡‡æ ·ç‡
          // ç”Ÿæˆå¤åˆæ³¢å½¢ï¼šåŸºé¢‘ + è°æ³¢ + å™ªå£°
          const wave = 0.3 * Math.sin(2 * Math.PI * 440 * t) +      // 440HzåŸºé¢‘
                      0.2 * Math.sin(2 * Math.PI * 880 * t) +      // ç¬¬äºŒè°æ³¢
                      0.1 * Math.sin(2 * Math.PI * 1320 * t) +     // ç¬¬ä¸‰è°æ³¢
                      0.05 * (Math.random() - 0.5)                 // å™ªå£°
          testSamples[i] = wave
        }
        
        waveformBufferRef.current.set(timeKey, testSamples)
        
        // å†…å­˜ç®¡ç†
        if (waveformBufferRef.current.size > 500) {
          const oldestKey = Math.min(...Array.from(waveformBufferRef.current.keys()))
          waveformBufferRef.current.delete(oldestKey)
        }
      }
      
      if (isRecording) {
        setTimeout(generateTestWaveform, 10)
      }
    }
    
    generateTestWaveform()
  }

  const stopRecording = () => {
    console.log('ğŸ›‘ Stopping timedomain recording for track', trackId)
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = undefined
    }
    
    if (microphoneRef.current) {
      microphoneRef.current.disconnect()
      microphoneRef.current = undefined
    }
    
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = undefined
    }
  }

  const draw = () => {
    const canvas = canvasRef.current
    if (!canvas) return
    
    const ctx = canvas.getContext('2d')
    if (!ctx) return
    
    // è®¾ç½®canvasåˆ†è¾¨ç‡
    const dpr = window.devicePixelRatio || 1
    canvas.width = width * dpr
    canvas.height = height * dpr
    ctx.scale(dpr, dpr)
    
    // æ¸…é™¤ç”»å¸ƒ
    ctx.fillStyle = '#1e293b' // slate-800
    ctx.fillRect(0, 0, width, height)
    
    // ç»˜åˆ¶ä¸­å¿ƒçº¿
    ctx.strokeStyle = '#475569' // slate-600
    ctx.lineWidth = 0.5
    ctx.beginPath()
    ctx.moveTo(0, height / 2)
    ctx.lineTo(width, height / 2)
    ctx.stroke()
    
    // è°ƒè¯•ä¿¡æ¯
    ctx.fillStyle = '#64748b'
    ctx.font = '10px monospace'
    ctx.textAlign = 'left'
    ctx.fillText(`Armed: ${isArmed}, Recording: ${isRecording}, Data chunks: ${waveformBufferRef.current.size}`, 5, 15)
    
    // å¦‚æœæ²¡æœ‰å½•éŸ³æ•°æ®ï¼Œæ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
    if (waveformBufferRef.current.size === 0) {
      ctx.fillStyle = isArmed ? '#22c55e' : '#64748b'
      ctx.font = '12px sans-serif'
      ctx.textAlign = 'center'
      ctx.fillText(
        isArmed ? (isRecording ? 'Recording... (waiting for data)' : 'Ready to record') : 'Arm track to record',
        width / 2,
        height / 2 + 4
      )
      return
    }
    
    // ç®€åŒ–çš„æ³¢å½¢ç»˜åˆ¶ç®—æ³•
    const centerY = height / 2
    const amplitudeScale = height * 0.4
    
    ctx.strokeStyle = '#22c55e'
    ctx.lineWidth = 1.5
    
    // è·å–æ‰€æœ‰æ—¶é—´é”®å¹¶æ’åº
    const timeKeys = Array.from(waveformBufferRef.current.keys()).sort((a, b) => a - b)
    
    // ä¸ºæ¯ä¸ªæ—¶é—´æ®µç»˜åˆ¶æ³¢å½¢
    for (const relativeTime of timeKeys) {
      const waveformSlice = waveformBufferRef.current.get(relativeTime)
      if (!waveformSlice) continue
      
      // è®¡ç®—è¿™ä¸ªæ—¶é—´æ®µåœ¨ç”»å¸ƒä¸Šçš„èµ·å§‹ä½ç½®
      const absoluteTime = recordingStartTimeRef.current + relativeTime
      const startX = (absoluteTime / duration) * width
      
      // å¦‚æœè¶…å‡ºå¯è§†èŒƒå›´ï¼Œè·³è¿‡
      if (startX >= width || startX < -100) continue
      
      // è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨xè½´ä¸Šçš„é—´è·
      const timeSpan = 0.01 // æ¯ä¸ªchunkä»£è¡¨0.01ç§’
      const pixelSpan = (timeSpan / duration) * width
      const pixelsPerSample = pixelSpan / waveformSlice.length
      
      // ç»˜åˆ¶è¿™ä¸ªchunkçš„æ³¢å½¢
      ctx.beginPath()
      let firstPoint = true
      
      for (let i = 0; i < waveformSlice.length; i += Math.max(1, Math.floor(waveformSlice.length / 200))) {
        const x = startX + (i * pixelsPerSample)
        if (x < 0 || x > width) continue
        
        const sample = waveformSlice[i]
        const y = centerY - (sample * amplitudeScale)
        
        if (firstPoint) {
          ctx.moveTo(x, y)
          firstPoint = false
        } else {
          ctx.lineTo(x, y)
        }
      }
      ctx.stroke()
    }
    
    // ç»˜åˆ¶å½“å‰æ’­æ”¾ä½ç½®æŒ‡ç¤ºå™¨
    const playheadX = (currentTime / duration) * width
    if (playheadX >= 0 && playheadX <= width) {
      ctx.strokeStyle = '#ef4444' // red-500
      ctx.lineWidth = 2
      ctx.beginPath()
      ctx.moveTo(playheadX, 0)
      ctx.lineTo(playheadX, height)
      ctx.stroke()
      
      if (isRecording) {
        // å½•éŸ³æŒ‡ç¤ºç‚¹
        ctx.fillStyle = '#ef4444'
        ctx.beginPath()
        ctx.arc(playheadX, centerY, 3, 0, 2 * Math.PI)
        ctx.fill()
      }
    }
  }

  return (
    <canvas
      ref={canvasRef}
      style={{ 
        width: `${width}px`, 
        height: `${height}px`,
        display: 'block'
      }}
      className="bg-slate-800 rounded"
    />
  )
}
