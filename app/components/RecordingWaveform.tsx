'use client'

import { useEffect, useRef, useCallback } from 'react'

interface WaveformBucket {
  min: number
  max: number
  rms: number
}

interface RecordingWaveformProps {
  isRecording: boolean
  isArmed: boolean
  trackId: string
  width: number
  height: number
  currentTime: number
  duration: number
}

export default function RecordingWaveform({
  isRecording,
  isArmed,
  trackId,
  width,
  height,
  currentTime,
  duration
}: RecordingWaveformProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const animationRef = useRef<number | null>(null)
  // å­˜å‚¨åˆ†æ¡¶åçš„æ³¢å½¢æ•°æ®ï¼šæ—¶é—´ -> æ³¢å½¢æ¡¶æ•°ç»„
  const waveformBucketsRef = useRef<Map<number, WaveformBucket[]>>(new Map())
  const recordingStartTimeRef = useRef<number>(0)
  // å½“å‰å®æ—¶éŸ³é¢‘æ•°æ®
  const currentAudioDataRef = useRef<Float32Array>(new Float32Array(0))
  // é‡‡æ ·ç¼“å†²åŒº
  const sampleBufferRef = useRef<Float32Array>(new Float32Array(0))

  // åˆ›å»ºæ³¢å½¢æ¡¶
  const createWaveformBuckets = (audioData: Float32Array, bucketCount: number): WaveformBucket[] => {
    const buckets: WaveformBucket[] = []
    const samplesPerBucket = Math.floor(audioData.length / bucketCount)
    
    for (let i = 0; i < bucketCount; i++) {
      const start = i * samplesPerBucket
      const end = Math.min(start + samplesPerBucket, audioData.length)
      
      let min = 0, max = 0, sum = 0
      for (let j = start; j < end; j++) {
        const sample = audioData[j]
        min = Math.min(min, sample)
        max = Math.max(max, sample)
        sum += sample * sample
      }
      
      buckets.push({
        min,
        max,
        rms: Math.sqrt(sum / (end - start))
      })
    }
    
    return buckets
  }

  // ç»˜åˆ¶æ³¢å½¢
  const drawWaveform = useCallback(() => {
    const canvas = canvasRef.current
    if (!canvas) return

    const ctx = canvas.getContext('2d')
    if (!ctx) return

    // æ¸…ç©ºç”»å¸ƒ
    ctx.fillStyle = '#1e293b' // slate-800
    ctx.fillRect(0, 0, width, height)

    // ç»˜åˆ¶ç½‘æ ¼çº¿
    ctx.strokeStyle = 'rgba(100, 116, 139, 0.2)' // slate-500 with opacity
    ctx.lineWidth = 1

    // å‚ç›´ç½‘æ ¼çº¿ (æ—¶é—´æ ‡è®°)
    const timeStep = Math.max(1, Math.floor(duration / 10)) // 10æ¡çº¿
    const pixelsPerSecond = width / Math.max(duration, 1)
    
    for (let i = 0; i <= duration; i += timeStep) {
      const x = i * pixelsPerSecond
      ctx.beginPath()
      ctx.moveTo(x, 0)
      ctx.lineTo(x, height)
      ctx.stroke()
    }

    // æ°´å¹³ä¸­å¿ƒçº¿
    const centerY = height / 2
    ctx.strokeStyle = 'rgba(100, 116, 139, 0.4)'
    ctx.beginPath()
    ctx.moveTo(0, centerY)
    ctx.lineTo(width, centerY)
    ctx.stroke()

    // ç»˜åˆ¶æ’­æ”¾å¤´ä½ç½®
    if (currentTime >= 0) {
      const playheadX = (currentTime / Math.max(duration, 1)) * width
      ctx.strokeStyle = '#ef4444' // red-500
      ctx.lineWidth = 2
      ctx.beginPath()
      ctx.moveTo(playheadX, 0)
      ctx.lineTo(playheadX, height)
      ctx.stroke()
    }

    // å¦‚æœæ­£åœ¨å½•éŸ³ä¸”è½¨é“å·²armedï¼Œç»˜åˆ¶å®æ—¶æ³¢å½¢
    if (isRecording && isArmed && analyserRef.current) {
      drawRealtimeWaveform(ctx, pixelsPerSecond, centerY)
      
      // æ·»åŠ è°ƒè¯•ä¿¡æ¯
      ctx.fillStyle = '#22c55e'
      ctx.font = '10px monospace'
      ctx.textAlign = 'left'
      ctx.fillText('â— LIVE', 5, 15)
    }

    // ç»˜åˆ¶å·²å½•åˆ¶çš„æ³¢å½¢æ•°æ®
    if (waveformBucketsRef.current.size > 0) {
      drawRecordedWaveform(ctx, pixelsPerSecond, centerY)
    }

    // çŠ¶æ€æç¤º
    if (!isRecording && isArmed) {
      // å¾…æœºçŠ¶æ€
      ctx.fillStyle = 'rgba(255, 165, 0, 0.5)' // orange
      ctx.font = '14px sans-serif'
      ctx.textAlign = 'center'
      ctx.fillText('READY TO RECORD', width / 2, height / 2 - 10)
    } else if (!isArmed) {
      // æœªarmedçŠ¶æ€
      ctx.fillStyle = 'rgba(100, 116, 139, 0.7)' // slate-500
      ctx.font = '12px sans-serif'
      ctx.textAlign = 'center'
      ctx.fillText('Track not armed for recording', width / 2, height / 2)
    }
  }, [width, height, currentTime, duration, isRecording, isArmed])

  // ç»˜åˆ¶å®æ—¶æ³¢å½¢
  const drawRealtimeWaveform = (ctx: CanvasRenderingContext2D, pixelsPerSecond: number, centerY: number) => {
    if (!analyserRef.current) return

    const bufferLength = analyserRef.current.fftSize
    const dataArray = new Float32Array(bufferLength)
    analyserRef.current.getFloatTimeDomainData(dataArray)

    // ç»˜åˆ¶å®Œæ•´çš„å®æ—¶æ³¢å½¢ï¼Œå æ»¡æ•´ä¸ªç”»å¸ƒå®½åº¦
    ctx.strokeStyle = '#22c55e' // green-500
    ctx.lineWidth = 2
    ctx.beginPath()

    const sliceWidth = width / bufferLength
    let x = 0

    for (let i = 0; i < bufferLength; i++) {
      const amplitude = dataArray[i]
      const y = centerY + (amplitude * (height * 0.8)) // 80% of height for better visibility
      
      if (i === 0) {
        ctx.moveTo(x, y)
      } else {
        ctx.lineTo(x, y)
      }
      x += sliceWidth
    }
    ctx.stroke()

    // å­˜å‚¨æ³¢å½¢æ•°æ®åˆ°å†å²è®°å½•
    const recordingTime = currentTime - recordingStartTimeRef.current
    if (recordingTime >= 0 && dataArray && dataArray.length > 0) {
      const timeKey = Math.floor(recordingTime * 10) / 10 // 0.1ç§’ç²¾åº¦
      const buckets = createWaveformBuckets(dataArray, Math.floor(width / 4)) // æ¯4åƒç´ ä¸€ä¸ªæ¡¶
      waveformBucketsRef.current.set(timeKey, buckets)
    }
  }

  // ç»˜åˆ¶å·²å½•åˆ¶çš„æ³¢å½¢
  const drawRecordedWaveform = (ctx: CanvasRenderingContext2D, pixelsPerSecond: number, centerY: number) => {
    ctx.strokeStyle = '#22c55e' // green-500
    ctx.lineWidth = 1

    Array.from(waveformBucketsRef.current.entries()).forEach(([timeKey, buckets]) => {
      const startX = timeKey * pixelsPerSecond
      
      if (startX >= 0 && startX < width) {
        // ç»˜åˆ¶æ³¢å½¢æ¡¶
        ctx.beginPath()
        buckets.forEach((bucket, index) => {
          const x = startX + (index * 4) // æ¯4åƒç´ ä¸€ä¸ªæ¡¶
          const minY = centerY - (bucket.min * (height * 0.4))
          const maxY = centerY - (bucket.max * (height * 0.4))
          
          if (index === 0) {
            ctx.moveTo(x, minY)
          } else {
            ctx.lineTo(x, minY)
          }
          ctx.lineTo(x, maxY)
        })
        ctx.stroke()
      }
    })
  }

  // å¯åŠ¨å½•éŸ³
  const startRecording = async () => {
    try {
      console.log('ğŸ¤ Starting audio recording for track:', trackId)
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 44100
        }
      })

      streamRef.current = stream
      audioContextRef.current = new AudioContext()
      analyserRef.current = audioContextRef.current.createAnalyser()
      
      analyserRef.current.fftSize = 2048  // ç»™æˆ‘ä»¬1024ä¸ªæ ·æœ¬ç‚¹
      analyserRef.current.smoothingTimeConstant = 0.1  // è½»å¾®å¹³æ»‘

      const source = audioContextRef.current.createMediaStreamSource(stream)
      source.connect(analyserRef.current)

      recordingStartTimeRef.current = currentTime
      waveformBucketsRef.current.clear()

      console.log('âœ… Audio recording started successfully')
      
      // å¼€å§‹æŒç»­çš„åŠ¨ç”»å¾ªç¯
      const animate = () => {
        drawWaveform()
        if (isRecording && isArmed && analyserRef.current) {
          animationRef.current = requestAnimationFrame(animate)
        }
      }
      animationRef.current = requestAnimationFrame(animate)

    } catch (error) {
      console.error('âŒ Error starting recording:', error)
      alert('æ— æ³•è®¿é—®éº¦å…‹é£ã€‚è¯·æ£€æŸ¥æƒé™è®¾ç½®ã€‚')
    }
  }

  // åœæ­¢å½•éŸ³
  const stopRecording = () => {
    console.log('ğŸ›‘ Stopping audio recording for track:', trackId)
    
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current)
      animationRef.current = null
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }

    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    analyserRef.current = null
  }

  // ç›‘å¬å½•éŸ³çŠ¶æ€å˜åŒ–
  useEffect(() => {
    if (isRecording && isArmed) {
      startRecording()
    } else {
      stopRecording()
    }

    return () => stopRecording()
  }, [isRecording, isArmed, trackId])

  // ç›‘å¬å°ºå¯¸å˜åŒ–
  useEffect(() => {
    const canvas = canvasRef.current
    if (canvas) {
      canvas.width = width
      canvas.height = height
      drawWaveform()
    }
  }, [width, height, drawWaveform])

  // ç›‘å¬æ—¶é—´å˜åŒ–ï¼Œé‡ç»˜æ’­æ”¾å¤´
  useEffect(() => {
    if (!isRecording) {
      drawWaveform()
    }
  }, [currentTime, drawWaveform])

  return (
    <canvas
      ref={canvasRef}
      width={width}
      height={height}
      className="w-full h-full block"
      style={{ 
        width: `${width}px`, 
        height: `${height}px`,
        background: '#1e293b'
      }}
    />
  )
}
